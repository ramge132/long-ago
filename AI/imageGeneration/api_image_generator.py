import os
import json
import base64
import asyncio
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass
from PIL import Image
import io
import requests
from openai import OpenAI
import google.generativeai as genai

# í™˜ê²½ë³€ìˆ˜ì—ì„œ API í‚¤ ê°€ì ¸ì˜¤ê¸°
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")

# ìºë¦­í„° íŒŒì¼ ê²½ë¡œ
CHARACTERS_DIR = os.path.join(os.path.dirname(__file__), "characters")

@dataclass
class Character:
    name: str
    image_path: str
    prompt: str

class CharacterManager:
    def __init__(self):
        self.characters: Dict[str, Character] = {}
        self.load_characters()
    
    def load_characters(self):
        """ìºë¦­í„° ì´ë¯¸ì§€ì™€ í”„ë¡¬í”„íŠ¸ ë¡œë“œ"""
        character_names = [
            "alien", "beggar", "boy", "detective", "doctor", 
            "farmer", "girl", "idol", "merchant", "ninja", 
            "oldman", "princess", "rich", "wizard"
        ]
        
        for name in character_names:
            image_path = os.path.join(CHARACTERS_DIR, f"{name}.png")
            txt_path = os.path.join(CHARACTERS_DIR, f"{name}.txt")
            
            if os.path.exists(image_path) and os.path.exists(txt_path):
                with open(txt_path, 'r', encoding='utf-8') as f:
                    prompt = f.read().strip()
                
                self.characters[name] = Character(
                    name=name,
                    image_path=image_path,
                    prompt=prompt
                )
    
    def get_character(self, name: str) -> Optional[Character]:
        """ìºë¦­í„° ì •ë³´ ê°€ì ¸ì˜¤ê¸°"""
        return self.characters.get(name)
    
    def get_all_character_names(self) -> List[str]:
        """ëª¨ë“  ìºë¦­í„° ì´ë¦„ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜"""
        return list(self.characters.keys())
    
    def detect_characters_in_text(self, text: str) -> List[str]:
        """í…ìŠ¤íŠ¸ì—ì„œ ì–¸ê¸‰ëœ ìºë¦­í„° íƒì§€"""
        detected = []
        text_lower = text.lower()
        
        # í•œêµ­ì–´-ì˜ì–´ ë§¤í•‘
        korean_mapping = {
            "ë‹Œì": "ninja", "ê³µì£¼": "princess", "ì˜ì‚¬": "doctor", 
            "ë†ë¶€": "farmer", "ë§ˆë²•ì‚¬": "wizard", "ìƒì¸": "merchant",
            "ì†Œë…„": "boy", "ì†Œë…€": "girl", "íƒì •": "detective",
            "ê±°ì§€": "beggar", "ë¶€ì": "rich", "ë…¸ì¸": "oldman",
            "ì•„ì´ëŒ": "idol", "ì™¸ê³„ì¸": "alien"
        }
        
        # í•œêµ­ì–´ ë‹¨ì–´ íƒì§€
        for korean, english in korean_mapping.items():
            if korean in text:
                detected.append(english)
        
        # ì˜ì–´ ë‹¨ì–´ íƒì§€
        for character_name in self.characters.keys():
            if character_name in text_lower:
                detected.append(character_name)
        
        return list(set(detected))  # ì¤‘ë³µ ì œê±°

class GPTPromptGenerator:
    def __init__(self):
        if not OPENAI_API_KEY:
            raise ValueError("OPENAI_API_KEY environment variable is not set")
        
        self.client = OpenAI(api_key=OPENAI_API_KEY)
        
        # ê¸°ì¡´ AYL.pyì˜ í”„ë¡¬í”„íŠ¸ ì‹œìŠ¤í…œì„ ê·¸ëŒ€ë¡œ í™œìš©
        self.story_prompt = """
You are an AI assistant specialized in summarizing stories in English and Korean stories currently entered. 
Your mission is to fully grasp the context of the summarized story and the Korean story currently entered, and then update the summarized English story. 
1. Mandatory Format: You must print only the summary story. 
2. Summarization of Inputs: - Continuously build a coherent summary of the story based on all user inputs up to the current point. 
   - Ensure that each subject appears only once in the [Subject] component, eliminating any duplicates. 
3. Missing or Partial Information: - If the user's input lacks certain components (e.g., no explicit Time), use context from the summarized story to maintain a natural, cohesive narrative. 
4. Strict Limitations: - Do not introduce new story elements beyond what the user provides. 
   - Do not modify or extend the user's narrative beyond summarization. 
   - Use only the content explicitly stated by the user, plus any relevant context from the summarized story to fill in missing details. 
# Important: - Add new information without ever deleting existing information. 
- Type of Input: Summarized story, current user input. 
- Output format: a summary of the summarized story and the current user's input.
"""
        
        self.description_prompt = """
You are an AI that synthesizes input data to produce an up-to-date, concise, and detailed 'Description' in English.

Follow these structured steps:

1. Main Subjects:
   - Identify all key subjects (characters, animals, objects).
   - For each subject, you MUST include the following sub-points:
     * Physical/External Features:
       - Keep the consistent external traits from older data (e.g., clothing style, color scheme, weapons/items, basic body shape).
       - Only remove or modify these if the newest user input explicitly changes them.
     * Emotional Expressions:
       - Overwrite older emotional states with the newest user input. 
       - If there's any conflict, the latest info prevails.
     * Actions/Poses:
       - Similarly, disregard older action/pose details unless they remain valid and do not contradict new info.
       - Focus on the new or current actions the subject is taking as per the latest input.
     * Relative Positioning:
       - This is CRITICAL for multiple characters. Indicate precisely how each subject is positioned relative to others (foreground, background, left/right, distance, etc.).
       - If the newest input changes the subject's location, update accordingly and remove outdated references.

2. Environment:
   - Briefly describe location (forest, city, battlefield, etc.), time/weather, and dynamic elements (shifting shadows, falling leaves, etc.).
   - Use older environment details ONLY if still valid. Remove duplicates or contradictory info.

3. Mood & Narrative:
   - Reflect the overall atmosphere and how the new user input changes or intensifies it.
   - Describe tensions, relationships, conflicts, or potential resolutions, but focus on the immediate scenario. 
   - Do NOT re-summarize the entire past storylineâ€”only keep what is relevant for the current moment.

4. Formatting & Constraints:
   - Structure your output in sections: Main Subjects (with the sub-points for each subject), Environment, Mood & Narrative.
   - Do NOT create an overall structure summary or retell the entire past events.
   - If older details conflict with the newest user input, remove or revise them. 
   - Keep the text concise, ensuring no repeated or redundant statements from previous descriptions.
   - The final goal is a coherent snapshot of the scene that prioritizes the latest user input for all dynamic aspects (emotion, action, position).

IMPORTANT:
- You will receive a 'Previous Description' plus new user input. 
- Extract only the stable external features from older data, and override all emotional/positional/action details with the new input.
- This ensures a fresh, updated Description without outdated or duplicate info.
"""
        
        self.image_prompt = """
You are an AI that takes the newly updated 'Description' and converts it into a single ultra-detailed cinematic scene in English.

Please adhere to the following:

1. Input Description:
   - The Description already has each subject's current emotional state, actions/poses, relative positioning, and any relevant environment or mood details.
   - Respect these updated details strictly. If older references conflict, ignore them.

2. Output Format:
   - Produce exactly ONE cohesive paragraph.
   - Begin with: "ultra-detailed cinematic scene," 
   - Then depict the scenario with strong, dynamic verbs and vivid sensory details (motion blur, lighting, sounds, environmental interactions).

3. Key Instructions:
   - Preserve each subject's external appearance from the updated Description (e.g., clothing style, color, weapons).
   - Use the newest emotional expressions, actions, poses, and relative positions as given.
   - Ensure that multiple subjects do not merge; maintain their distinct positions via consistent references (e.g., "in the foreground," "to the left," "several paces away," etc.).
   - Highlight transitional motion effects (dust trails, shockwaves, etc.) and emphasize changes signaled by the latest input.

4. Constraints:
   - DO NOT repeat older or contradictory text from previous versions. 
   - Avoid bullet points or separate headingsâ€”only one cinematic paragraph.
   - If there's any mention in the updated Description about changes to a subject's state or location, reflect that precisely. 
   - The focus is on the current moment. Avoid rehashing prior events or extended backstory.

Goal:
- Deliver a powerful, immersive snapshot that shows exactly how the scene looks and feels after the latest changes, with correct subject positions, actions, and emotions.
"""
    
    def generate_story_summary(self, previous_summary: str, user_input: str) -> str:
        """ìŠ¤í† ë¦¬ ìš”ì•½ ìƒì„±"""
        combined_input = f"Story_summary: {previous_summary}\nUser_Input: {user_input}"
        
        try:
            response = self.client.chat.completions.create(
                model="gpt-4o-mini",  # GPT-5-nanoê°€ ì•„ì§ ì—†ìœ¼ë¯€ë¡œ gpt-4o-mini ì‚¬ìš©
                messages=[
                    {"role": "system", "content": self.story_prompt},
                    {"role": "user", "content": combined_input}
                ],
                max_tokens=4096,
                temperature=0.7
            )
            return response.choices[0].message.content.strip()
        except Exception as e:
            print(f"Error in story summary generation: {e}")
            return previous_summary
    
    def generate_description(self, previous_description: str, story_summary: str, user_input: str) -> str:
        """ì¥ë©´ ì„¤ëª… ìƒì„±"""
        combined_input = f"Story summary: {story_summary}\nDescription: {previous_description}\nCurrent story: {user_input}"
        
        try:
            response = self.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": self.description_prompt},
                    {"role": "user", "content": combined_input}
                ],
                max_tokens=4096,
                temperature=0.7
            )
            return response.choices[0].message.content.strip()
        except Exception as e:
            print(f"Error in description generation: {e}")
            return previous_description
    
    def generate_image_prompt(self, description: str, user_input: str) -> str:
        """ì´ë¯¸ì§€ ìƒì„±ìš© í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        combined_input = f"Description: {description}\nCurrent story: {user_input}"
        
        try:
            response = self.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": self.image_prompt},
                    {"role": "user", "content": combined_input}
                ],
                max_tokens=4096,
                temperature=0.7
            )
            return response.choices[0].message.content.strip()
        except Exception as e:
            print(f"Error in image prompt generation: {e}")
            return description

class GeminiImageGenerator:
    def __init__(self):
        if not GEMINI_API_KEY:
            raise ValueError("GEMINI_API_KEY environment variable is not set")
        
        genai.configure(api_key=GEMINI_API_KEY)
        self.model = genai.GenerativeModel("gemini-2.5-flash-image-preview")
    
    def encode_image_to_base64(self, image_path: str) -> str:
        """ì´ë¯¸ì§€ë¥¼ base64ë¡œ ì¸ì½”ë”©"""
        with open(image_path, "rb") as image_file:
            return base64.b64encode(image_file.read()).decode('utf-8')
    
    def load_image_for_gemini(self, image_path: str):
        """Geminiìš© ì´ë¯¸ì§€ ë¡œë“œ"""
        with open(image_path, "rb") as image_file:
            image_data = image_file.read()
        
        # PIL Image ê°ì²´ë¡œ ë³€í™˜
        image = Image.open(io.BytesIO(image_data))
        return image
    
    def generate_text_to_image(self, prompt: str, art_style: str = "") -> bytes:
        """í…ìŠ¤íŠ¸ì—ì„œ ì´ë¯¸ì§€ ìƒì„± (ìºë¦­í„° ì—†ëŠ” ê²½ìš°)"""
        full_prompt = f"{prompt} {art_style}".strip()
        
        try:
            response = self.model.generate_content([
                f"Generate an image: {full_prompt}"
            ])
            
            # ì‘ë‹µì—ì„œ ì´ë¯¸ì§€ ë°ì´í„° ì¶”ì¶œ
            if response.candidates and response.candidates[0].content.parts:
                for part in response.candidates[0].content.parts:
                    if hasattr(part, 'inline_data'):
                        return base64.b64decode(part.inline_data.data)
            
            raise Exception("No image data in response")
            
        except Exception as e:
            print(f"Error in text-to-image generation: {e}")
            raise
    
    def generate_image_to_image(self, character_images: List[str], prompt: str, character_prompts: List[str], art_style: str = "") -> bytes:
        """ì´ë¯¸ì§€ì—ì„œ ì´ë¯¸ì§€ ìƒì„± (ìºë¦­í„° ìˆëŠ” ê²½ìš°)"""
        try:
            # ìºë¦­í„° ì´ë¯¸ì§€ë“¤ì„ ë¡œë“œ
            images = []
            for img_path in character_images:
                if os.path.exists(img_path):
                    images.append(self.load_image_for_gemini(img_path))
            
            # ìºë¦­í„° í”„ë¡¬í”„íŠ¸ë“¤ì„ ê²°í•©
            combined_character_prompt = " ".join(character_prompts)
            
            # ì „ì²´ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            full_prompt = f"Generate an image based on these reference images. {combined_character_prompt} {prompt} {art_style}".strip()
            
            # ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ë¥¼ í•¨ê»˜ ì „ë‹¬
            content = [full_prompt] + images
            
            response = self.model.generate_content(content)
            
            # ì‘ë‹µì—ì„œ ì´ë¯¸ì§€ ë°ì´í„° ì¶”ì¶œ
            if response.candidates and response.candidates[0].content.parts:
                for part in response.candidates[0].content.parts:
                    if hasattr(part, 'inline_data'):
                        return base64.b64decode(part.inline_data.data)
            
            raise Exception("No image data in response")
            
        except Exception as e:
            print(f"Error in image-to-image generation: {e}")
            raise

class APIImageGenerationSystem:
    def __init__(self):
        self.character_manager = CharacterManager()
        self.gpt_generator = GPTPromptGenerator()
        self.gemini_generator = GeminiImageGenerator()
        
        # ê·¸ë¦¼ì²´ë³„ ìŠ¤íƒ€ì¼ í”„ë¡¬í”„íŠ¸ (ê¸°ì¡´ LoRA ëŒ€ì²´)
        self.art_styles = {
            0: "anime style, vibrant colors, detailed illustration",
            1: "cute 3d cartoon style, soft colors, rounded features",
            2: "comic strip style, bold outlines, dramatic expressions",
            3: "claymation style, 3D rendered, soft clay texture",
            4: "crayon drawing style, childlike, soft pastels",
            5: "pixel art style, retro gaming aesthetic, sharp pixels",
            6: "minimalist illustration, clean lines, simple colors",
            7: "watercolor painting style, soft blending, artistic",
            8: "storybook illustration, whimsical, detailed"
        }
    
    async def generate_image(self, 
                           user_input: str, 
                           game_mode: int, 
                           session_data: Dict) -> Tuple[bytes, Dict]:
        """
        ë©”ì¸ ì´ë¯¸ì§€ ìƒì„± í•¨ìˆ˜
        
        Args:
            user_input: ì‚¬ìš©ì ì…ë ¥ í…ìŠ¤íŠ¸
            game_mode: ê²Œì„ ëª¨ë“œ (0-8, ê·¸ë¦¼ì²´ ê²°ì •)
            session_data: ì„¸ì…˜ ë°ì´í„° (prev_prompt, summary, description í¬í•¨)
        
        Returns:
            Tuple[bytes, Dict]: (ì´ë¯¸ì§€ ë°”ì´íŠ¸ ë°ì´í„°, ì—…ë°ì´íŠ¸ëœ ì„¸ì…˜ ë°ì´í„°)
        """
        
        # 1. GPTë¥¼ í†µí•œ í”„ë¡¬í”„íŠ¸ ìƒì„±
        print("ğŸ”¹ [GPT] ìŠ¤í† ë¦¬ ìš”ì•½ ìƒì„± ì¤‘...")
        story_summary = self.gpt_generator.generate_story_summary(
            session_data.get("summary", ""), 
            user_input
        )
        
        print("ğŸ”¹ [GPT] ì¥ë©´ ì„¤ëª… ìƒì„± ì¤‘...")
        description = self.gpt_generator.generate_description(
            session_data.get("description", ""),
            story_summary,
            user_input
        )
        
        print("ğŸ”¹ [GPT] ì´ë¯¸ì§€ í”„ë¡¬í”„íŠ¸ ìƒì„± ì¤‘...")
        image_prompt = self.gpt_generator.generate_image_prompt(description, user_input)
        
        # 2. ìºë¦­í„° íƒì§€
        detected_characters = self.character_manager.detect_characters_in_text(user_input)
        print(f"ğŸ”¹ [ìºë¦­í„° íƒì§€] ë°œê²¬ëœ ìºë¦­í„°: {detected_characters}")
        
        # 3. ê·¸ë¦¼ì²´ ìŠ¤íƒ€ì¼ ê°€ì ¸ì˜¤ê¸°
        art_style = self.art_styles.get(game_mode, "")
        
        # 4. ì´ë¯¸ì§€ ìƒì„±
        if detected_characters:
            # ìºë¦­í„°ê°€ ìˆëŠ” ê²½ìš°: image-to-image ëª¨ë“œ
            print("ğŸ”¹ [Gemini] Image-to-Image ëª¨ë“œë¡œ ì´ë¯¸ì§€ ìƒì„± ì¤‘...")
            character_images = []
            character_prompts = []
            
            for char_name in detected_characters:
                character = self.character_manager.get_character(char_name)
                if character:
                    character_images.append(character.image_path)
                    character_prompts.append(character.prompt)
            
            image_bytes = self.gemini_generator.generate_image_to_image(
                character_images, image_prompt, character_prompts, art_style
            )
        else:
            # ìºë¦­í„°ê°€ ì—†ëŠ” ê²½ìš°: text-to-image ëª¨ë“œ
            print("ğŸ”¹ [Gemini] Text-to-Image ëª¨ë“œë¡œ ì´ë¯¸ì§€ ìƒì„± ì¤‘...")
            image_bytes = self.gemini_generator.generate_text_to_image(
                image_prompt, art_style
            )
        
        # 5. ì„¸ì…˜ ë°ì´í„° ì—…ë°ì´íŠ¸
        updated_session_data = {
            "prev_prompt": image_prompt,
            "summary": story_summary,
            "description": description
        }
        
        print("âœ… [ì´ë¯¸ì§€ ìƒì„± ì™„ë£Œ]")
        return image_bytes, updated_session_data

# ì‚¬ìš© ì˜ˆì‹œ ë° í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
async def test_system():
    """ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸"""
    system = APIImageGenerationSystem()
    
    # í…ŒìŠ¤íŠ¸ ì„¸ì…˜ ë°ì´í„°
    session_data = {
        "prev_prompt": "",
        "summary": "",
        "description": ""
    }
    
    # í…ŒìŠ¤íŠ¸ 1: ìºë¦­í„° ì—†ëŠ” ê²½ìš°
    print("=== í…ŒìŠ¤íŠ¸ 1: ìºë¦­í„° ì—†ëŠ” ê²½ìš° ===")
    try:
        image_bytes, updated_data = await system.generate_image(
            "ì•„ë¦„ë‹¤ìš´ ìˆ²ì†ì— í–‡ë¹›ì´ ë¹„ì¹˜ê³  ìˆë‹¤", 
            0,  # ì• ë‹ˆë©”ì´ì…˜ ìŠ¤íƒ€ì¼
            session_data
        )
        print(f"ìƒì„±ëœ ì´ë¯¸ì§€ í¬ê¸°: {len(image_bytes)} bytes")
        session_data = updated_data
    except Exception as e:
        print(f"í…ŒìŠ¤íŠ¸ 1 ì‹¤íŒ¨: {e}")
    
    # í…ŒìŠ¤íŠ¸ 2: ìºë¦­í„° ìˆëŠ” ê²½ìš°
    print("=== í…ŒìŠ¤íŠ¸ 2: ìºë¦­í„° ìˆëŠ” ê²½ìš° ===")
    try:
        image_bytes, updated_data = await system.generate_image(
            "ë‹Œìê°€ ìˆ²ì†ì—ì„œ ìˆ˜í–‰ì„ í•˜ê³  ìˆë‹¤", 
            1,  # 3D ì¹´íˆ° ìŠ¤íƒ€ì¼
            session_data
        )
        print(f"ìƒì„±ëœ ì´ë¯¸ì§€ í¬ê¸°: {len(image_bytes)} bytes")
        session_data = updated_data
    except Exception as e:
        print(f"í…ŒìŠ¤íŠ¸ 2 ì‹¤íŒ¨: {e}")

if __name__ == "__main__":
    asyncio.run(test_system())